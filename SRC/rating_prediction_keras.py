# -*- coding: utf-8 -*-
"""ds4002dsp1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qw2-CxIMr1z8xOcrCxlhbE_gMuCoRpLg
"""

!pip install tensorflow

import pandas as pd
import numpy as np
import tensorflow as tf
import keras

from sklearn.model_selection import train_test_split

from keras.models import Sequential
from keras.layers import Dense, TextVectorization, Dropout
from keras.losses import CategoricalCrossentropy
import matplotlib.pyplot as plt

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

def read_data(filename):
    input = pd.read_csv(filename).rename(
        columns={"reviews.text": "text", "reviews.rating": "rating"})
    data = input[input['text'].notna() & input['rating'].notna()][['text','rating']]
    data['text'] = data['text'].apply(
        lambda x: " ".join([word for word in x.split() if 
                            word.lower() not in stop_words and len(word) > 2]))
    
    return train_test_split(data, test_size = 0.1, random_state=0)

def create_vectorize_layer(x_train, vector_type, vocab_size):


    vectorize_layer = TextVectorization(max_tokens = vocab_size, 
                                        output_mode = vector_type)
    
    x_train_tf = tf.data.Dataset.from_tensor_slices(x_train)
    vectorize_layer.adapt(x_train_tf.batch(64))
    # print(vectorize_layer.get_vocabulary())
    return vectorize_layer

def create_mlp(args = None):
    model = Sequential()
    model.add(keras.Input(shape=(1,), dtype=tf.string))
    model.add(args['vectorize_layer'])
    for layer in args['layers']:
        model.add(Dense(units=layer,activation='relu'))
        model.add(Dropout(0.2))
    model.add(Dense(units=5, activation='softmax'))

    optimizer = keras.optimizers.Adam(learning_rate=args['learning rate'])
    model.compile(loss=CategoricalCrossentropy(), optimizer=optimizer, metrics=['accuracy'])
    
    return model

def train_mlp(x_train, y_train, args = None):
    y_train = keras.utils.to_categorical(y_train - 1, num_classes = 5)
    model = create_mlp(args)
    history = model.fit(x_train, y_train, epochs = args['epoch'], 
                        batch_size = args['batch size'],
                        validation_split = args['validation_split'])

    return model, history

def train_model(reviews_file, vectorizer):
    train_reviews, test_reviews = read_data(reviews_file)

    args = {
    'validation_split': 0.2,
            'epoch': 50,
            'learning rate': 0.0001,
            'batch size': 128,
            'vectorize_layer': create_vectorize_layer(
                train_reviews['text'], vectorizer, 500),
            'layers': [256,256,128]
    }

    best_model = None
    best_valid_acc = 0
    best_hyper_set = None

    # print('Parameters: ', args)

    model, history = train_mlp(
        train_reviews['text'], train_reviews['rating'], args)
    validation_accuracy = max(history.history['val_accuracy'])
    plot_history(history)

    if validation_accuracy > best_valid_acc:
        best_model = model
        best_valid_acc = validation_accuracy
        best_hyper_set = args
        best_history = history

    return best_model, best_history, best_hyper_set

def plot_history(history):
    
    def plot(train, val, kind):

        x = np.arange(1, np.size(train) + 1)

        plt.plot(x, train, label = "training " + kind)
        plt.plot(x, val, label = "validation " + kind)
        plt.xlabel("epoch")
        plt.ylabel(kind)
        plt.title("training and validation " + kind + " v epoch")
        plt.show()
        
    train_loss_history = history.history['loss']
    val_loss_history = history.history['val_loss']

    train_acc_history = history.history['accuracy']
    val_acc_history = history.history['val_accuracy']

    # plot
    plot(train_loss_history, val_loss_history, 'loss')
    plot(train_acc_history, val_acc_history, 'accuracy')

bow_model, bow_model_history, bow_model_hyper_set = train_model('consumer.csv','count')
tfidf_model, tfidf_model_history, tfidf_model_hyper_set = train_model('consumer.csv','tf_idf')

def analyze_model(test, model, history):
    print(model.summary())
    plot_history(history)
    predicted = np.argmax(model.predict(test['text']), axis = 1) + 1
    test_df = pd.DataFrame([test['text'],test['rating'],predicted], 
                           columns = ['Review','Rating', 'Predicted Rating'])
    test_df['Difference'] = test_df['Rating'] - test_df['Predicted Rating']
    test_df['Prediction Correct'] = test_df['Difference'] == 0
    prediction_accuracy = test_df['Prediction Correct'].mean()
    print('Prediction Accuracy of Model: ', prediction_accuracy)

    difference_freq = test['Difference'].value_counts()
    print(difference_freq)

    prediction_by_label = test_df.groupby('Rating')['Prediction Correct'].sum()
    print(prediction_by_label)





